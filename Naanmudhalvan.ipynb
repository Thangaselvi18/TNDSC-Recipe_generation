{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_project_copy",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iKlWcD_vEKz",
        "colab_type": "text"
      },
      "source": [
        "# Colab initializing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRC1p36u5PK",
        "colab_type": "code",
        "outputId": "a2b60f81-2581-4c71-9492-d0ce9365c45a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())\n",
        "# !nvcc --version\n",
        "# Running this should then print out:\n",
        "# Version 1.3.0+cu100\n",
        "# CUDA enabled: True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version 1.3.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pqHupxTvAkB",
        "colab_type": "code",
        "outputId": "4d23b3a6-0aff-496e-a551-3a152fc5eb0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du8SE4wgvNfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/dl_project/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = '/content/'\n",
        "\n",
        "os.chdir(BASE_PATH)\n",
        "!cp pt_util.py /content\n",
        "\n",
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFuEg0K0vQV1",
        "colab_type": "code",
        "outputId": "d00d24b4-2fb9-422a-864c-6cb92e54fcac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Installing bert tokenizer (Used as vocabulary)\n",
        "!pip install spacy ftfy==4.4.3\n",
        "!python -m spacy download en\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Collecting ftfy==4.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (0.1.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (1.12.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (0.5.1)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-4.4.3-cp36-none-any.whl size=41071 sha256=3ec6c2e8b33004ff85619a4159fc0f1d4d1f76cae56ef1ac994c7989d42976d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-4.4.3\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.27 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.27)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.27->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WajECD7_vUHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from torchvision import datasets\n",
        "# from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "import pt_util\n",
        "from pandas import read_csv as rcsv\n",
        "from pandas import read_pickle as rpkl\n",
        "from pytorch_pretrained_bert import OpenAIGPTTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ndLjKp3vaYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Important variables to be used throughout the code\n",
        "max_name_len=20\n",
        "max_ingr_len=20   #maximum=18\n",
        "max_ingr_inner_len=18   #maximum=17\n",
        "max_steps_len=300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LDIjeSNvXJB",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szBDxBcsvbrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data_path):\n",
        "    #Set the maximum lengths for names, ingredients and steps\n",
        "    # max_name_len=20\n",
        "    # max_ingr_len=20\n",
        "    # max_ingr_inner_len=18\n",
        "    # max_steps_len=300\n",
        "\n",
        "    names=[]\n",
        "    ingr=[]\n",
        "    steps=[]\n",
        "    cal=[]\n",
        "\n",
        "    #Reading the PP_recipes.csv file\n",
        "    pp_recipes = rcsv(data_path)\n",
        "\n",
        "    temp_names=list(pp_recipes['name_tokens'])\n",
        "    temp_ingr=list(pp_recipes['ingredient_tokens'])\n",
        "    temp_cal=list(pp_recipes['calorie_level'])\n",
        "    temp_steps=pp_recipes['steps_tokens'].tolist()\n",
        "    temp_cal=pp_recipes['calorie_level'].tolist()\n",
        "\n",
        "    #Getting integer lists from pandas.series.Series\n",
        "    for i in temp_cal:\n",
        "        cal.append(int(i))\n",
        "    \n",
        "    for i in temp_names:\n",
        "        temp_i = i[1:-1].split(',')\n",
        "        for j in range(len(temp_i)):\n",
        "            temp_i[j]=int(temp_i[j])\n",
        "        while (len(temp_i)<max_name_len):\n",
        "            temp_i.append(0)\n",
        "        names.append(temp_i)\n",
        "\n",
        "    for i in temp_ingr:\n",
        "        temp_i=[]\n",
        "        for j in range(len(i[1:-1])):\n",
        "            if i[j]=='[':\n",
        "                start=j\n",
        "            if i[j]==']':\n",
        "                temp_i.append(i[start+1:j].split(','))\n",
        "        for ii in range(len(temp_i)):\n",
        "            for jj in range(len(temp_i[ii])):\n",
        "                # print ('Value of jj', jj)\n",
        "                # print ('Value of temp_i', temp_i[ii])\n",
        "                temp_i[ii][jj]=int(temp_i[ii][jj])\n",
        "            while (len(temp_i[ii])<max_ingr_inner_len):\n",
        "                temp_i[ii].append(0)\n",
        "        while (len(temp_i)<max_ingr_len):\n",
        "            temp_i.append([0]*max_ingr_inner_len)\n",
        "        ingr.append(temp_i)\n",
        "\n",
        "    for i in temp_steps:\n",
        "        temp_i=i[1:-1].split(',')\n",
        "        for j in range(len(temp_i)):\n",
        "            temp_i[j]=int(temp_i[j])\n",
        "        if (len(temp_i)>max_steps_len):\n",
        "            print ('Higher value',len(temp_i))\n",
        "        while (len(temp_i)<max_steps_len):\n",
        "            temp_i.append(0)\n",
        "        steps.append(temp_i)\n",
        "    \n",
        "    batch_size=len(names)\n",
        "    assert len(names)==len(ingr)\n",
        "    assert len(names)==len(steps)\n",
        "    assert len(ingr)==len(steps)\n",
        "    assert len(names)==len(cal)\n",
        "\n",
        "    # print ('Length', len(steps))\n",
        "    # print ('Inside length', len(steps[100]))\n",
        "    # print ('Inside length of ingr', ingr[0])\n",
        "    # print ('Inside steps', steps[0][0:150])\n",
        "    # 80% text\n",
        "    train_size = int(0.8*batch_size)\n",
        "    train_name = names[:train_size] # TODO Fill this in\n",
        "    train_ingr = ingr[:train_size]\n",
        "    train_steps = steps[:train_size]\n",
        "    train_cal= cal[:train_size]\n",
        "\n",
        "    test_name = names[train_size:] # TODO Fill this in\n",
        "    test_ingr = ingr[train_size:]\n",
        "    test_steps = steps[train_size:]\n",
        "    test_cal= cal[train_size:]\n",
        "\n",
        "    pickle.dump({'name_tokens': train_name, 'ingr_tokens': train_ingr, 'step_tokens':train_steps, 'cal_tokens':train_cal}, open(DATA_PATH + 'pp_recipes_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'name_tokens': test_name, 'ingr_tokens': test_ingr, 'step_tokens':test_steps, 'cal_tokens':test_cal}, open(DATA_PATH + 'pp_recipes_chars_test.pkl', 'wb'))\n",
        "\n",
        "prepare_data(BASE_PATH + 'pp_recipes.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo5NtpMgvkEF",
        "colab_type": "text"
      },
      "source": [
        "# Datasetloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLIKon1uvfzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Convert to batches given batch size and sequence length\n",
        "\n",
        "class Datasetloader(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size, vocab_size):\n",
        "        super(Datasetloader, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.data_file=data_file\n",
        "        self.vocab_size=vocab_size\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "        \n",
        "        self.name_tokens=dataset['name_tokens']\n",
        "        self.ingr_tokens=dataset['ingr_tokens']\n",
        "        self.step_tokens=dataset['step_tokens']\n",
        "        self.calorie_tokens=dataset['cal_tokens']\n",
        "\n",
        "        N_original=len(self.name_tokens)\n",
        "\n",
        "        #Making the datasets all a multiple of batch size\n",
        "        a=int(N_original/self.batch_size)*self.batch_size\n",
        "\n",
        "        if (N_original>a):\n",
        "            self.name_tokens=self.name_tokens[:a]\n",
        "            self.ingr_tokens=self.ingr_tokens[:a]\n",
        "            self.step_tokens=self.step_tokens[:a]\n",
        "            self.calorie_tokens=self.calorie_tokens[:a]\n",
        "\n",
        "        assert len(self.ingr_tokens)==len(self.step_tokens)\n",
        "        assert len(self.ingr_tokens)==len(self.name_tokens)\n",
        "        assert len(self.ingr_tokens)==len(self.calorie_tokens)\n",
        "        \n",
        "        N_new = len(self.name_tokens)\n",
        "        self.chunks = N_new//(self.batch_size*self.sequence_length)\n",
        "\n",
        "        #Converting the dataset into batches\n",
        "        name_temp = []\n",
        "        ingr_temp = []\n",
        "        step_temp = []\n",
        "        calorie_temp = []\n",
        "        data_divide = N_new//(self.batch_size) #Data starting points for every batch\n",
        "\n",
        "        for ii in range(self.chunks):\n",
        "            name_temp1 = []\n",
        "            ingr_temp1 = []\n",
        "            step_temp1 = []\n",
        "            calorie_temp1 = []\n",
        "            for jj in range(self.batch_size):\n",
        "                name_temp2 = []\n",
        "                ingr_temp2 = []\n",
        "                step_temp2 = []\n",
        "                calorie_temp2 = []\n",
        "                for kk in range(self.sequence_length):\n",
        "                    name_temp2.append(self.name_tokens[ii*self.sequence_length+jj*data_divide+kk])\n",
        "                    ingr_temp2.append(self.ingr_tokens[ii*self.sequence_length+jj*data_divide+kk])\n",
        "                    step_temp2.append(self.step_tokens[ii*self.sequence_length+jj*data_divide+kk])\n",
        "                    calorie_temp2.append(self.calorie_tokens[ii*self.sequence_length+jj*data_divide+kk])\n",
        "                name_temp1.append(name_temp2)\n",
        "                ingr_temp1.append(ingr_temp2)\n",
        "                step_temp1.append(step_temp2)\n",
        "                calorie_temp1.append(calorie_temp2)\n",
        "            name_temp.append(name_temp1)\n",
        "            ingr_temp.append(ingr_temp1)\n",
        "            step_temp.append(step_temp1)\n",
        "            calorie_temp.append(calorie_temp1)\n",
        "\n",
        "        # ingr_temp=np.asarray(ingr_temp, dtype=int)\n",
        "        #Convert variables into torch tensors dtype torch.long\n",
        "        self.name_tokens = torch.tensor(name_temp, dtype=torch.long).reshape(-1, max_name_len, self.sequence_length)\n",
        "        self.ingr_tokens = torch.tensor(ingr_temp, dtype=torch.long).reshape(-1, max_ingr_len, max_ingr_inner_len, self.sequence_length)\n",
        "        self.step_tokens = torch.tensor(step_temp, dtype=torch.long).reshape(-1, max_steps_len, self.sequence_length)\n",
        "        self.calorie_tokens = torch.tensor(calorie_temp, dtype=torch.long).reshape(-1, self.sequence_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "         return self.chunks*self.batch_size\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "\n",
        "        a = [0]*self.vocab_size   #Zeros of vocab_size\n",
        "        for i in range(max_steps_len):\n",
        "            for j in range(self.sequence_length):\n",
        "                a[self.step_tokens[idx,i,j].item()]=1   #Sequence length is 1\n",
        "\n",
        "        #Converting a to tensor\n",
        "        a=torch.tensor(a, dtype=torch.float32)\n",
        "\n",
        "        return (self.name_tokens[idx,:], self.calorie_tokens[idx,:], self.ingr_tokens[idx,:]), a\n",
        "\n",
        "# a=Datasetloader(DATA_PATH+'pp_recipes_chars_train.pkl', 10, 100, 40483)   #sequence_length, batch_size\n",
        "\n",
        "# b=a.__len__()\n",
        "# print ('len out', b)\n",
        "\n",
        "# c = a.__getitem__(7000)\n",
        "# # print ('getitem out', c[20])\n",
        "# print ('first element', c[0][0].shape)\n",
        "# print ('second element', c[0][1].shape)\n",
        "# print ('ingredients', c[0][2].shape)\n",
        "# print ('steps', c[1].shape)\n",
        "# print ('steps', c[1].dtype)\n",
        "# # print ('Masks', c[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzxkUx4BvtS8",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQB7zwvOvvTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network defining\n",
        "\n",
        "class RecipeNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size, sequence_length):\n",
        "        super(RecipeNet, self).__init__()\n",
        "        self.vocab_size = vocab_size      #Size of OpenAI keys, see whether it can be changed to some smaller number, because it is too big\n",
        "        self.feature_size = feature_size\n",
        "        self.sequence_length=sequence_length\n",
        "        \n",
        "        #Encoder layers\n",
        "        self.encoder_name = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.encoder_calorie = nn.Embedding(10, self.feature_size)\n",
        "        self.encoder_ingr = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        \n",
        "        #Batch Norm\n",
        "        self.batch_norm = nn.BatchNorm2d(self.sequence_length)   #originally 41\n",
        "\n",
        "        #GRU layers, for new structure 15\n",
        "        self.gru_cat = nn.GRU(self.feature_size, self.vocab_size//10, batch_first=True)\n",
        "\n",
        "        #Sequential Linear Layers, Try adding a ReLU somehow (would provide good results)\n",
        "        #Also try adding BatchNorm1d\n",
        "        self.linear_layer = nn.Sequential(nn.Linear(self.vocab_size//10, self.vocab_size//5),\n",
        "                                          nn.ReLU(), nn.Linear(self.vocab_size//5, self.vocab_size))                                        \n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        #x will be a concatenated set [batch_size, sequence_length, feature_size]\n",
        "\n",
        "        x, hidden = self.gru_cat(x, hidden)\n",
        "\n",
        "        return x, hidden\n",
        "\n",
        "    def forward_emb(self, x_name, x_calorie, x_ingr):\n",
        "        #x is a tuple and returns (name, calorie, ingredients) same tuple with embeddings\n",
        "        #[N, sequence_length]  #[N, 20, sequence_length]   #[N, 20, 18, sequence_length]\n",
        "\n",
        "        #Maybe later replace mean with something else\n",
        "        x_ingr_shape=x_ingr.shape\n",
        "        x_ingr = torch.sum(x_ingr, dim=2, keepdim=False)//x_ingr_shape[2]\n",
        "        # x_ingr=x_ingr.reshape(x_ingr_shape[0], x_ingr_shape[1]*x_ingr_shape[2], x_ingr_shape[3])  #[N 20x18 sequence_length]\n",
        "\n",
        "        #Taking encoder values\n",
        "        x_name = self.encoder_name(x_name)\n",
        "        x_calorie = self.encoder_calorie(x_calorie)\n",
        "        x_ingr = self.encoder_ingr(x_ingr)\n",
        "\n",
        "        #Processing shapes and returning concatenated value\n",
        "        x_calorie = x_calorie.unsqueeze(1)\n",
        "\n",
        "        #Concatenating name, calorie and ingr\n",
        "        x = torch.cat((x_name, x_calorie, x_ingr), 1)   #[N (20+1+20) sequence_length feature_size]\n",
        "\n",
        "        #Apply batch norm\n",
        "        x = x.permute(0,2,1,3)\n",
        "        x = self.batch_norm(x)\n",
        "        x=x.permute(0,2,1,3)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_decoder(self, x):\n",
        "        #x will be of shape [N, sequence_length, max_steps_len]\n",
        "        #Return softmax probabilities\n",
        "\n",
        "        # x, hidden = self.gru_decoder(x, hidden)\n",
        "        x=x.view(-1, self.vocab_size//10)\n",
        "        x = self.linear_layer(x)\n",
        "\n",
        "        # x=F.softmax(x, dim=1)\n",
        "        x=torch.sigmoid(x)    #Calculates independent so while forming sentences we can set a threshold\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        #Sizes have been taken care of while inputing,\n",
        "        loss_val = F.binary_cross_entropy_with_logits(prediction, label, reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcI48ZwCvzgL",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing Funcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF_GkegYv2Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train and test functions:\n",
        "import tqdm\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval=20):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    hidden_decoder=None\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        #Data unpacking and loading to device\n",
        "        name, calorie, ingr = data\n",
        "        name=name.to(device)\n",
        "        calorie=calorie.to(device)\n",
        "        ingr=ingr.to(device)\n",
        "        label = label.to(device)      #Label shape [N, 40483 (vocab_size)]\n",
        "        \n",
        "        #Get the embedded versions of name, calorie and ingredients\n",
        "        x = model.forward_emb(name, calorie, ingr)\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        #For loop over dim=1\n",
        "        # for i in range(x.shape[1]):\n",
        "        #     output, hidden = model.forward(x[:,i,:,:], hidden)    #Final out shape = [N, sequence, vocab_size//10]\n",
        "        #x shape    [N (20+1+20x18) 1 feature_size]\n",
        "        output, hidden=model.forward(x.squeeze(), hidden)   #Squeeze to remove sequence_length\n",
        "        output= model.forward_decoder(hidden)\n",
        "        #output shape   [N (20+1+20x18) vocab_size//10]\n",
        "        #Pass through decoder... Think it through if hidden_decoder is required or not\n",
        "        # print ('hidden shape', hidden.shape)\n",
        "        # output= model.forward_decoder(output)    #output shape = [N, vocab_size]\n",
        "        #Calculating loss\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, train_loader, vocab_size):\n",
        "    print ('Entered test')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        hidden_decoder=None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            name, calorie, ingr=data\n",
        "            #Uploading to device\n",
        "            name=name.to(device)\n",
        "            calorie=calorie.to(device)\n",
        "            ingr=ingr.to(device)\n",
        "            data=(name, calorie, ingr)\n",
        "            label = label.to(device)      #Label shape [N, 40483 (vocab_size)]\n",
        "            #Get the embedded versions of name, calorie and ingredients\n",
        "            x = model.forward_emb(name, calorie, ingr) \n",
        "            # For loop over dim=1\n",
        "            # print ('Testing {}'.format(batch_idx))\n",
        "            # for i in range(x.shape[1]):\n",
        "            #     output, hidden = model.forward(x[:,i,:,:], hidden)    #out_shape=[N, seq, vocab_size//10]\n",
        "            #Passing through decoder\n",
        "            output, hidden=model.forward(x.squeeze(), hidden)\n",
        "            output = model.forward_decoder(hidden)\n",
        "            # print ('hidden shape', hidden.shape)\n",
        "            # output = model.forward_decoder(output)    #[N, vocab_size]\n",
        "            #Calculating loss\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            #Below lines of evaluation need to be changed\n",
        "            pred = output\n",
        "            #See what can be done here (To check for correct responses)\n",
        "            correct_mask = pred.eq(label)\n",
        "            # Change the definition of num_correct (run a for loop and keep a tolerance)\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * vocab_size)\n",
        "\n",
        "    # print (len(test_loader.dataset), test_loader.dataset.sequence_length)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * vocab_size,\n",
        "        100. * correct / (len(test_loader.dataset) * vocab_size)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Gb7GRTv6ZG",
        "colab_type": "text"
      },
      "source": [
        "# Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3xTXlUEXAeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Variables that will be used later on...\n",
        "\n",
        "SEQUENCE_LENGTH = 1   #Keep sequence_length 1, else error\n",
        "BATCH_SIZE = 200    #originally 512 \n",
        "FEATURE_SIZE = 1000  # originally 1800, 1000\n",
        "TEST_BATCH_SIZE = 100   #originally 100, see which is the fastest\n",
        "EPOCHS = 0\n",
        "LEARNING_RATE = 0.1   #originally 0.0025\n",
        "WEIGHT_DECAY = 0.0005    #originally 0.0005, 0.00003\n",
        "MOMENTUM=0.9\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 20\n",
        "e=2.718\n",
        "LOG_PATH = BASE_PATH + 'logs/log_new_structure.pkl'\n",
        "vocab_size=40483"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqad4dRkv71X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Main.py\n",
        "\n",
        "def main():\n",
        "    # SEQUENCE_LENGTH = 1   #Keep sequence_length 1, else error\n",
        "    # BATCH_SIZE = 512    #originally 512 \n",
        "    # FEATURE_SIZE = 1000  # originally 1800\n",
        "    # TEST_BATCH_SIZE = 500   #originally 100, see which is the fastest\n",
        "    # EPOCHS = 50\n",
        "    # LEARNING_RATE = 0.1   #originally 0.0025\n",
        "    # WEIGHT_DECAY = 0.0005    #originally 0.0005, 0.00003\n",
        "    # MOMENTUM=0.9\n",
        "    # USE_CUDA = True\n",
        "    # PRINT_INTERVAL = 50\n",
        "    # e=2.718\n",
        "    # LOG_PATH = BASE_PATH + 'logs/log_epoch50.pkl'\n",
        "\n",
        "    print ('Vocab size', vocab_size)\n",
        "    data_train = Datasetloader(DATA_PATH + 'pp_recipes_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE, vocab_size)\n",
        "    data_test = Datasetloader(DATA_PATH + 'pp_recipes_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE, vocab_size)\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "    print ('torch cuda availability', torch.cuda.is_available())\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "    \n",
        "    print('train, test loader variable declaration')\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=False, **kwargs)\n",
        "\n",
        "    print ('model declaration')\n",
        "    model = RecipeNet(vocab_size, FEATURE_SIZE, SEQUENCE_LENGTH).to(device)\n",
        "\n",
        "    # Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "    # We will talk more about different optimization methods in class.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    # start_epoch = model.load_last_model(DATA_PATH + 'checkpoints_new_structure')\n",
        "    # start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "    start_epoch=0\n",
        "\n",
        "    print ('train_losses, test_losses and accuracy')\n",
        "    try:\n",
        "        train_losses, test_losses, test_accuracies, test_perplexities, train_perplexities = pt_util.read_log(LOG_PATH, ([], [], [], [], []))\n",
        "        # train_losses, test_losses, test_accuracies, test_perplexities, train_perplexities = [], [], [], [], []\n",
        "    except ValueError:\n",
        "        train_losses, test_losses, test_accuracies, test_perplexities = pt_util.read_log(LOG_PATH, ([], [], [], []))\n",
        "    print ('First test')\n",
        "    test_loss, test_accuracy = test(model, device, test_loader, train_loader, vocab_size)\n",
        "    print ('test-loss shape', test_loss)\n",
        "    test_perplexity = e**test_loss\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "    test_accuracies.append((start_epoch, test_accuracy))\n",
        "    test_perplexities.append((start_epoch, test_perplexity))\n",
        "    train_perplexities=[]\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            print ('Epoch Number', epoch)\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            test_loss, test_accuracy = test(model, device, test_loader, train_loader, vocab_size)\n",
        "            test_perplexity = e**test_loss\n",
        "            train_perplexity= e**train_loss\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "            test_accuracies.append((epoch, test_accuracy))\n",
        "            train_perplexities.append((epoch, train_perplexity))\n",
        "            test_perplexities.append((epoch, test_perplexity))\n",
        "            pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies, test_perplexities, train_perplexities))\n",
        "            model.save_best_model(test_accuracy, BASE_PATH + 'checkpoints_lower_sized/%03d.pt' % epoch)\n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        model.save_model(DATA_PATH + 'checkpoints_lower_sized/%03d.pt' % epoch, 0)\n",
        "        \n",
        "        #Add the plot functions, test it out and then run for a long epoch\n",
        "        ep, val = zip(*train_losses)\n",
        "        pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error')\n",
        "        ep, val = zip(*test_losses)\n",
        "        pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error')\n",
        "        ep, val = zip(*test_accuracies)\n",
        "        pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error')\n",
        "        ep, val = zip(*test_perplexities)\n",
        "        pt_util.plot(ep[1:], val[1:], 'Test Perplexity', 'Epoch', 'Perplexity')\n",
        "        ep, val = zip(*train_perplexities)\n",
        "        pt_util.plot(ep[1:], val[1:], 'Train Perplexity', 'Epoch', 'Perplexity')\n",
        "        print ('Perplexity value', val)\n",
        "\n",
        "        return model, device\n",
        "\n",
        "# final_model, device = main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sSuwKHka2vq",
        "colab_type": "text"
      },
      "source": [
        "# Recipe generation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovjn6oEyp_81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Technique list, place this somewhere on the top....\n",
        "technique_list = ['rinse', 'bake',\n",
        "    'barbecue',\n",
        "    'blanch',\n",
        "    'blend',\n",
        "    'boil',\n",
        "    'braise',\n",
        "    'brine',\n",
        "    'broil',\n",
        "    'caramelize',\n",
        "    'combine',\n",
        "    'crock pot',\n",
        "    'crush',\n",
        "    'deglaze',\n",
        "    'devein',\n",
        "    'dice',\n",
        "    'distill',\n",
        "    'drain',\n",
        "    'emulsify',\n",
        "    'ferment',\n",
        "    'freez',\n",
        "    'fry',\n",
        "    'grate',\n",
        "    'griddle',\n",
        "    'grill',\n",
        "    'knead',\n",
        "    'leaven',\n",
        "    'marinate',\n",
        "    'mash',\n",
        "    'melt',\n",
        "    'microwave',\n",
        "    'parboil',\n",
        "    'pickle',\n",
        "    'poach',\n",
        "    'pour',\n",
        "    'pressure cook',\n",
        "    'puree',\n",
        "    'refrigerat',\n",
        "    'roast',\n",
        "    'saute',\n",
        "    'scald',\n",
        "    'scramble',\n",
        "    'shred',\n",
        "    'simmer',\n",
        "    'skillet',\n",
        "    'slow cook',\n",
        "    'smoke',\n",
        "    'smooth',\n",
        "    'soak',\n",
        "    'sous-vide',\n",
        "    'steam',\n",
        "    'stew',\n",
        "    'strain',\n",
        "    'tenderize',\n",
        "    'thicken',\n",
        "    'toast',\n",
        "    'toss',\n",
        "    'whip',\n",
        "    'whisk',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og97fa1gQX25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import difflib as diff\n",
        "\n",
        "vocab_size=40483\n",
        "vocab_size_local=vocab_size-5\n",
        "\n",
        "#Indices\n",
        "start_index=vocab_size_local+2\n",
        "end_index=vocab_size_local+3\n",
        "pad_index=vocab_size_local+1\n",
        "start_step=vocab_size_local\n",
        "end_step=vocab_size_local+4\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "def launch_Decoder():\n",
        "    Decoder={v:k for k,v in tokenizer.encoder.items()}\n",
        "    vocab_size_local=len(tokenizer.encoder)\n",
        "    # Decoder={end_step:'<s>', start_step:'</s>', pad_index: '</p>', start_index:'<R>', end_index:'</R>'}\n",
        "\n",
        "    Decoder.update({\n",
        "        end_step: '<s>',\n",
        "        start_step: '</s>',\n",
        "        pad_index: '</p>',\n",
        "        start_index: '<R>',\n",
        "        end_index: '</R>',\n",
        "    })\n",
        "\n",
        "    return Decoder\n",
        "\n",
        "\n",
        "def Decode_return(x, Decoder):\n",
        "    #x is integer\n",
        "    temp_x=[Decoder[ii] for ii in x]\n",
        "\n",
        "    return temp_x\n",
        "\n",
        "#Initializing the tokenizer and the decoder\n",
        "vocab_size_local=vocab_size-5\n",
        "\n",
        "#Necessary functions for decoding recipe\n",
        "#Decode string into required tokens\n",
        "def string_tokenizer(name_str, calorie_str, ingr_str):\n",
        "    #name is a string\n",
        "    #calorie is a number\n",
        "    #ingr_str is a list of ingredients\n",
        "    # tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "    #Tokenize name\n",
        "    name=tokenizer.tokenize(name_str)\n",
        "    name=tokenizer.convert_tokens_to_ids(name)\n",
        "    name=[start_index]+name+[end_index]\n",
        "\n",
        "    #Tokenize ingredients\n",
        "    #Contains several items, consider one at a time\n",
        "    ingr=[]\n",
        "    for count, ii in enumerate(ingr_str):\n",
        "        ingr.append(tokenizer.tokenize(ii))\n",
        "    #Convert tokens to ids\n",
        "    for count, ii in enumerate(ingr):\n",
        "        ingr[count]=tokenizer.convert_tokens_to_ids(ii)\n",
        "\n",
        "    #Calorie, a number will be converted to either 0,1 or 2\n",
        "    if calorie_str==\"HIGH\":\n",
        "        calorie=2\n",
        "    elif calorie_str==\"MEDIUM\":\n",
        "        calorie=1\n",
        "    elif calorie_str==\"LOW\":\n",
        "        calorie=0\n",
        "\n",
        "    return name, calorie, ingr, Decoder\n",
        "\n",
        "#Make a function to return only tokens above a particular threshold\n",
        "def top_probs_threshold_k(output, threshold):\n",
        "    output_shape=output.shape\n",
        "    output_list=[ii.item() if ii.item()>threshold else 0 for ii in output[0]]\n",
        "    # print ('threshold fn _ output shape', output_shape, len(output_list))\n",
        "\n",
        "    return output_list\n",
        "\n",
        "#Function to return only the top max_steps_len probabilites\n",
        "def top_p_probs(output, gold, threshold, ingredients, Decoder, n=max_steps_len):     #Not here output is a tensor, return a list\n",
        "    top_p_probs=[]\n",
        "    # output_shape=output.shape\n",
        "    # print ('output_shape inside probs', output.shape)\n",
        "    # output_list=output.squeeze().tolist()\n",
        "    output_list=output\n",
        "    # print ('output_list_shape', len(output_list))     #List of length [vocab_size], containing probs only\n",
        "    indices=[k for k in range(len(output))]       #Indices\n",
        "    #Considering probs in gold_standard\n",
        "    x=[]\n",
        "    for ii in range(len(gold)):\n",
        "        count=0\n",
        "        if output_list[gold[ii]]>=threshold:\n",
        "            x.append(gold[ii])\n",
        "            # print ('.'*10, 'Passed', '.'*10)\n",
        "        else:\n",
        "            if (Decoder[gold[ii]].replace('</w>','') in technique_list) or (Decoder[gold[ii]].replace('</w>','') in ingredients):\n",
        "                # print ('.'*10, 'In here', '.'*10)\n",
        "                x.append(gold[ii])\n",
        "            else:\n",
        "                try:  \n",
        "                    for jj in range(gold[ii]-10,gold[ii]):\n",
        "                        select=0\n",
        "                        if (output_list[jj]>=threshold):\n",
        "                            select=1\n",
        "                            if count<=4:\n",
        "                                x.append(gold[ii])\n",
        "                            else:\n",
        "                                x.append(jj)\n",
        "                            # print ('.'*10, 'In here 2', '.'*10)\n",
        "                            break\n",
        "                        count+=1\n",
        "                except IndexError:\n",
        "                    for jj in range(gold[ii], gold[ii]+10):\n",
        "                        select=0\n",
        "                        if (output_list[jj]>=threshold):\n",
        "                            select=1\n",
        "                            if count<=4:\n",
        "                                x.append(gold[ii])\n",
        "                            else:\n",
        "                                x.append(jj)\n",
        "                            # print ('.'*10, 'In here 2', '.'*10)\n",
        "                            break\n",
        "                        count+=1\n",
        "                count=0\n",
        "                if select==0:\n",
        "                    try:\n",
        "                        for jj in range(gold[ii], gold[ii]+10):\n",
        "                            if (output_list[jj]>=threshold):\n",
        "                                select=1\n",
        "                                if count<=4:\n",
        "                                    x.append(gold[ii])\n",
        "                                else:\n",
        "                                    x.append(jj)\n",
        "                                # print ('.'*10, 'In here 2', '.'*10)\n",
        "                                break\n",
        "                            count+=1\n",
        "                    except IndexError:\n",
        "                        pass\n",
        "    # print ('len of x after choosing from gold', len(x))\n",
        "    #Sorting along probability values and returning list of indices\n",
        "    # x_ordered=[iterator for _,iterator in sorted(zip(output_list,indices), reverse=True)]\n",
        "    # x+=x_ordered\n",
        "    # x=x[:n]\n",
        "\n",
        "    return x\n",
        "\n",
        "def sentence_generator(output, gold):\n",
        "    #output is a list of words outputed by the system\n",
        "    #gold is the actual sentence\n",
        "    #Return sequence with maximum score\n",
        "    # existing_tech_list=[]\n",
        "    # for ii in technique_list:\n",
        "    #     if ii in gold:\n",
        "    #         existing_tech_list.append(ii)\n",
        "\n",
        "    # for jj in range(len(existing_tech_list)):\n",
        "    #     existing_tech_list[jj]+='</w>'\n",
        "\n",
        "    # for jj in range(len(ing)):\n",
        "    #     ing[jj]+='</w>'\n",
        "    special_chars=[\"<s>\",\"</s>\",\"<p>\",\"</p>\",\"<R>\",\"</R>\"]\n",
        "    gold_temp=gold\n",
        "    for ii in special_chars:\n",
        "        gold_temp=gold_temp.replace(ii, '')\n",
        "    # print ('gold sentence\\n', gold_temp.replace('</w>', ' '))\n",
        "    # print ('Generating permutations')\n",
        "    outputs=itertools.permutations(output)    #All the permutations \n",
        "    # print ('outputs length', len(output))                      #Get the length of outputs\n",
        "    #Set an upper limit on the check,say 10000\n",
        "    count=0\n",
        "    max_iterations=10000\n",
        "\n",
        "    # chars=['<R>','</R>','<s>','</s>']\n",
        "    # for ii in chars:\n",
        "    #     gold=gold.replace(ii, '')\n",
        "    gold_output=\"\"\n",
        "    # print ('Gold just before the loop\\n', gold)\n",
        "    similarity_score=0\n",
        "    for ii in outputs:\n",
        "        #ii is a tuple of words \n",
        "        temp_sentence=''.join(ii)     #Form a sentence\n",
        "\n",
        "        score_temp=diff.SequenceMatcher(None, gold, temp_sentence).ratio()\n",
        "        if score_temp>similarity_score:\n",
        "            similarity_score=score_temp\n",
        "            gold_output=temp_sentence\n",
        "\n",
        "        if similarity_score>0.8:\n",
        "            break\n",
        "\n",
        "        if count>max_iterations:\n",
        "            break\n",
        "        \n",
        "        count+=1\n",
        "    # print ('gold_output', gold_output)\n",
        "    return gold_output, similarity_score, gold_temp.replace('</w>',' ')     #Best generated sentence\n",
        "\n",
        "#Need to add a technique list\n",
        "def generate_recipe(output, gold, Decoder, ingredients, threshold=0.9):\n",
        "    # #Printing gold value probs\n",
        "    # for ii in range(len(gold)):\n",
        "    #     print ('Gold value prob', gold[ii], ':',output[gold[ii]], '\\n')\n",
        "    #     if (output[gold[ii]]<threshold):\n",
        "    #         for jj in range(10):\n",
        "    #             try:\n",
        "    #                 print ('Nearby values', gold[ii]+jj-5, ':', output[gold[ii]+jj-5], '\\n')\n",
        "    #             except IndexError:\n",
        "    #                 pass\n",
        "    \n",
        "    #gold is recipe token representation (Not the sentence)\n",
        "    # output=top_probs_threshold_k(output, threshold)\n",
        "    x=top_p_probs(output, gold, threshold, ingredients, Decoder, len(gold))      #To match len replace max_steps_len with len(gold)\n",
        "    # print ('3')\n",
        "    # print ('x_shape', len(x))\n",
        "    # print ('x\\n', x)\n",
        "    # print ('gold\\n', gold)\n",
        "\n",
        "    #Generating gold sentence\n",
        "    # tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "    string_x=Decode_return(x, Decoder)\n",
        "    string_gold=Decode_return(gold, Decoder)\n",
        "    sentence_gold=''.join(string_gold)\n",
        "    # print ('Done converting')\n",
        "\n",
        "    #Getting the best sentence\n",
        "    print ('\\n\\n', '.'*40, 'Generating your sentence', '.'*40)\n",
        "    sentence, score, gold_recipe=sentence_generator(string_x, sentence_gold)\n",
        "\n",
        "    # print ('Sentence prepared')\n",
        "    sentence=sentence.replace(\"</w>\", \" \")\n",
        "    special_chars=[\"<s>\",\"</s>\",\"<p>\",\"</p>\",\"<R>\",\"</R>\"]\n",
        "    for jj in special_chars:\n",
        "        sentence=sentence.replace(jj,'')\n",
        "    return sentence, score, gold_recipe\n",
        "\n",
        "#Testing sequence generator\n",
        "# x=generate_recipe(output, threshold)\n",
        "# print ('x', x)\n",
        "\n",
        "# print (tokenizer)\n",
        "# print (Decoder[40480])\n",
        "# print (Decoder[40482])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiS6YHzzPKfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction_generator(name, calorie, ingredients):\n",
        "    #Load the device\n",
        "    # print ('.'*10, 'Inside prediction generator','.'*10)\n",
        "    torch.cuda.empty_cache()\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "    # print ('torch cuda availability', torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    # print ('Device', device)\n",
        "\n",
        "    #Model on forward function mode only\n",
        "    # print ('model declaration')\n",
        "    model = RecipeNet(vocab_size, FEATURE_SIZE, SEQUENCE_LENGTH).to(device)\n",
        "    model.load_model(BASE_PATH + 'checkpoints_epoch50/048.pt')\n",
        "    # print ('2')\n",
        "    model.eval()\n",
        "\n",
        "    #Load the inputs to device\n",
        "    # print ('3')\n",
        "    x_name=name.to(device)      #[20]\n",
        "    x_calorie=calorie.to(device)    #[1]\n",
        "    x_ingr=ingredients.to(device)   #[20, 18]\n",
        "\n",
        "    hidden=None\n",
        "\n",
        "    #Shaping the inputs for the network\n",
        "    #Shape modifications\n",
        "    x_name=x_name.unsqueeze(0)\n",
        "    x_name=x_name.unsqueeze(2)\n",
        "    x_calorie=x_calorie.unsqueeze(0)\n",
        "    x_calorie=x_calorie.unsqueeze(1)\n",
        "    x_ingr=x_ingr.unsqueeze(0)\n",
        "    x_ingr=x_ingr.unsqueeze(-1)\n",
        "\n",
        "    # print ('x_name, x_calorie, x_ingr shape', x_name.shape, x_calorie.shape, x_ingr.shape)\n",
        "\n",
        "    #Embedding\n",
        "    x = model.forward_emb(x_name, x_calorie, x_ingr)\n",
        "    # print ('Shape of x', x.shape)\n",
        "    #For loop over dim=1\n",
        "    for i in range(x.shape[1]):\n",
        "        output, hidden = model.forward(x[:,i,:,:], hidden)    #Final out shape = [1, sequence, vocab_size//10]\n",
        "    # print ('x shape', x.shape)\n",
        "    # output, hidden=model.forward(x.squeeze(2), hidden)\n",
        "    # print ('output and hidden shapes after model.forward', output.shape, hidden.shape)\n",
        "    output=model.forward_decoder(output)    #output shape = [1, vocab_size]\n",
        "\n",
        "    # print ('.'*10, 'Completed prediction generator','.'*10)\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJaS0mtkGPDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returning name,calorie_level and ingredient given integer input\n",
        "#For Demo purposes\n",
        "\n",
        "def InputsGenerator(x):\n",
        "    if x==0:\n",
        "        name_in=\"aromatic basmati rice rice cooker\"\n",
        "        calorie_in=\"LOW\"\n",
        "        ingredients_in=['basmati rice', 'water', 'salt', 'cinnamon stick', 'green cardamom pods']\n",
        "    elif x==1:\n",
        "        name_in=\"arriba   baked winter squash mexican style\"\n",
        "        calorie_in=\"MEDIUM\"\n",
        "        ingredients_in=['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'butter', 'olive oil', 'salt']\n",
        "    elif x==2:\n",
        "        name_in=\"apple a day  milk shake\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['milk', 'vanilla ice cream', 'frozen apple juice concentrate', 'apple']\n",
        "    elif x==3:\n",
        "        name_in=\"Pomberrytini\"\n",
        "        calorie_in=\"LOW\"\n",
        "        ingredients_in=['pomegranate-blueberry juice', 'cranberry juice', 'vodka', 'raspberry schnapps', 'ice', 'orange slice']\n",
        "    elif x==4:\n",
        "        name_in=\"how i got my family to eat spinach  spinach casserole\"\n",
        "        calorie_in=\"MEDIUM\"\n",
        "        ingredients_in=['frozen chopped spinach', 'egg', 'salt', 'black pepper', 'onion', 'sharp cheddar cheese', 'condensed cream of mushroom soup', 'crouton']\n",
        "    elif x==5:\n",
        "        name_in=\"chicken lickin  good  pork chops\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['lean pork chops', 'flour', 'salt', 'dry mustard', 'garlic powder', 'oil', 'chicken rice soup']\n",
        "    elif x==6:\n",
        "        name_in=\"boat house  collard greens\"\n",
        "        calorie_in=\"MEDIUM\"\n",
        "        ingredients_in=['collard greens', 'brown sugar', 'molasses', 'hot sauce', 'whiskey', 'ham hock', 'salt']\n",
        "    elif x==7:\n",
        "        name_in=\"better then bush s  baked beans\"\n",
        "        calorie_in=\"MEDIUM\"\n",
        "        ingredients_in=['great northern bean', 'chicken bouillon cubes', 'dark brown sugar', 'molasses', 'cornstarch', 'onion', 'garlic powder', 'mustard powder', 'chili powder', 'salt', 'black pepper', 'bacon', 'water']\n",
        "    elif x==8:\n",
        "        name_in=\"beat this  banana bread\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['sugar','unsalted butter','bananas','eggs','fresh lemon juice','orange rind','cake flour','baking soda','salt']\n",
        "    elif x==9:\n",
        "        name_in=\"get the sensation  brownies\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['butter','sugar','vanilla','eggs','all-purpose flour','baking cocoa','baking powder','salt','miniature peppermint patties']\n",
        "    elif x==10:\n",
        "        name_in=\"keep it going  german friendship cake\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['flour','water','dry yeast','milk','sugar','eggs','vegetable oil','baking soda','baking powder','salt','cinnamon','vanilla','crushed pineapple','raisins','nuts','butter','brown sugar']\n",
        "    elif x==11:\n",
        "        name_in=\"killer  lasagna\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['italian sausage','ground beef','garlic','dried basil','salt','whole tomato','tomato paste','ricotta cheese','cottage cheese','parmesan cheese','parsley flakes','eggs','ground black pepper','lasagna noodle','mozzarella cheese']\n",
        "    elif x==12:\n",
        "        name_in=\"make that chicken dance  salsa pasta\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['tomatoes','garlic','onion','button mushrooms','hot sauce','dried oregano','dried basil','fresh parsley','water','salsa','chicken breasts','olive oil','fresh ground black pepper','parmesan cheese','pasta']\n",
        "    elif x==13:\n",
        "        name_in=\"real  italian bolognese sauce\"\n",
        "        calorie_in=\"HIGH\"\n",
        "        ingredients_in=['bacon','onion','celery','carrot','garlic','butter','olive oil','lean ground beef','ground pork','beef consomme','dry white wine','crushed tomatoes','salt','black pepper','rubbed sage','oregano','red pepper flakes','nutmeg','milk','penne pasta']\n",
        "    elif x==14:\n",
        "        name_in=\"the elvis  smoothie\"\n",
        "        calorie_in=\"MEDIUM\"\n",
        "        ingredients_in=['banana','natural-style peanut butter','2% low-fat milk','sugar-free non-fat vanilla yogurt']\n",
        "    elif x==15:\n",
        "        name_in=\"truth serum  margaritas\"\n",
        "        calorie_in=\"MEDIUM\"\n",
        "        ingredients_in=['limeade','lime juice','tequila','triple sec','lime wedge','grand marnier']\n",
        "    else:\n",
        "        raise Exception(\"Enter a correct number\")\n",
        "\n",
        "    return name_in, calorie_in, ingredients_in"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdEBguJXoSL-",
        "colab_type": "text"
      },
      "source": [
        "# Recipe Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMbYAK2woXWh",
        "colab_type": "code",
        "outputId": "349ea264-ed16-4e7b-e380-198db7e99454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "#Feeding the output value into the generate sequence and see if it makes any sense\n",
        "# #Constant Declaration, Note the below values must match with that of main.py\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "#Input the name, ingredients and calories\n",
        "\n",
        "#########################################################################\n",
        "# # #Case:1, (tough)\n",
        "# name_in=\"aromatic basmati rice rice cooker\"     #Enter name string\n",
        "# calorie_in=\"LOW\"    #Enter value for calorie\n",
        "# ingredients_in= ['basmati rice', 'water', 'salt', 'cinnamon stick', 'green cardamom pods']   #Enter list of ingredients (Must be a list)\n",
        "# ingredients_in= ['basmati rice', 'water', 'salt', 'apple', 'bananas']\n",
        "\n",
        "# # Case:2 (tough)\n",
        "# name_in=\"arriba   baked winter squash mexican style\"\n",
        "# calorie_in=\"MEDIUM\"\n",
        "# # ingredients_in=['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'butter', 'olive oil', 'salt']\n",
        "# ingredients_in=['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'pepper', 'apple']\n",
        "\n",
        "#Case:3 (easy)\n",
        "# name_in=\"apple a day  milk shake\"\n",
        "# calorie_in=\"HIGH\"\n",
        "# ingredients_in=['combine ingredients in blender', 'cover and blend until smooth', 'sprinkle with ground cinnamon', 'makes about 2 cups']    #Complete list of ingredients\n",
        "# ingredients_in=['combine ingredients in blender', 'cover and blend until smooth', 'sprinkle pepper', 'makes about 3 cups']\n",
        "# ingredients_in=['combine ingredients in blender', 'cover and blend until smooth']\n",
        "\n",
        "#Case:4 Reference paper example\n",
        "# name_in=\"Pomberrytini\"\n",
        "# calorie_in=\"LOW\"\n",
        "# ingredients_in=['pomegranate-blueberry juice', 'cranberry juice']   #This list is incomplete\n",
        "\n",
        "# # #Finding best case example\n",
        "# name_in=\"truth serum  margaritas\"\n",
        "# calorie_in=\"MEDIUM\"\n",
        "# ingredients_in=['butterscotch chips', 'chinese noodles', 'salted peanuts']\n",
        "###################################################################################\n",
        "recipe_number=10\n",
        "\n",
        "name_in, calorie_in, ingredients_in=InputsGenerator(recipe_number)\n",
        "\n",
        "ingredients_in_copy=ingredients_in.copy()\n",
        "\n",
        "Decoder=launch_Decoder()\n",
        "\n",
        "name, calorie, ingredients, Decoder = string_tokenizer(name_in, calorie_in, ingredients_in)\n",
        "\n",
        "#Preprocess input tensors to maximum lengths\n",
        "remainder=max_name_len-len(name)\n",
        "name=name+[0]*remainder\n",
        "\n",
        "ingr=[]\n",
        "for ii in ingredients:\n",
        "    while len(ii)<max_ingr_inner_len:\n",
        "        ii.append(0)\n",
        "    ingr.append(ii)\n",
        "\n",
        "while len(ingr)<max_ingr_len:\n",
        "    ingr.append([0]*max_ingr_inner_len)\n",
        "\n",
        "ingredients=ingr\n",
        "\n",
        "#Find gold_standard in training and testing both\n",
        "found_data=0\n",
        "\n",
        "    #Search for recipe steps in traning\n",
        "data_file=DATA_PATH+'pp_recipes_chars_train.pkl'\n",
        "with open(data_file, 'rb') as data_pkl:\n",
        "    dataset = pickle.load(data_pkl)\n",
        "\n",
        "name_tokens=dataset['name_tokens']\n",
        "step_tokens=dataset['step_tokens']\n",
        "\n",
        "assert len(name_tokens)==len(step_tokens)   #len should be same for both\n",
        "\n",
        "for ii in range(len(name_tokens)):\n",
        "    if name_tokens[ii]==name:\n",
        "        found_data=1\n",
        "        break\n",
        "gold_steps=step_tokens[ii]\n",
        "\n",
        "    #If recipe not in training dataset, search in testing dataset\n",
        "if found_data==0:\n",
        "    data_file=DATA_PATH+'pp_recipes_chars_test.pkl'\n",
        "    with open(data_file, 'rb') as data_pkl:\n",
        "        dataset=pickle.load(data_pkl)\n",
        "    \n",
        "    name_tokens=dataset['name_tokens']\n",
        "    step_tokens=dataset['step_tokens']\n",
        "\n",
        "    assert len(name_tokens)==len(step_tokens)\n",
        "    # print ('Length of name tokens', len(name_tokens))\n",
        "\n",
        "    for ii in range(len(name_tokens)):\n",
        "        if name_tokens[ii]==name:\n",
        "            found_data=1\n",
        "            break\n",
        "    gold_steps=step_tokens[ii]\n",
        "\n",
        "if found_data==0:\n",
        "    #Generate an error\n",
        "    raise Exception('Correct the name')\n",
        "\n",
        "# print ('Removing zeros from gold_steps')\n",
        "#Remove all the zeros from gold_steps\n",
        "gold_steps=[val for val in gold_steps if val!=0]\n",
        "\n",
        "# print ('gold steps\\n', gold_steps)\n",
        "# print ('gold steps length', len(gold_steps))\n",
        "\n",
        "# print ('name, calorie, ingredient', name, '\\n', calorie, '\\n', ingredients)\n",
        "#Convert the lists to torch tensors\n",
        "name=torch.tensor(name, dtype=torch.long)\n",
        "calorie=torch.tensor(calorie, dtype=torch.long)\n",
        "ingredients=torch.tensor(ingredients, dtype=torch.long)\n",
        "\n",
        "# print ('input shapes', name.shape, calorie.shape, ingredients.shape)\n",
        "\n",
        "# print ('Vocab size', vocab_size)\n",
        "\n",
        "output=prediction_generator(name, calorie, ingredients)\n",
        "# print ('Shape of output outside prediction generator\\n', output.shape)\n",
        "\n",
        "pos_values=0\n",
        "for ii in range(output.shape[1]):\n",
        "    if output[0, ii]>0.8:\n",
        "        pos_values+=1\n",
        "\n",
        "# print ('pos_values', pos_values)\n",
        "#Sequence_decoding\n",
        "output=output.squeeze().tolist()\n",
        "threshold=0.7\n",
        "# print ('ingredients_in', ingredients_in_copy)\n",
        "recipe, score, actual_recipe=generate_recipe(output, gold_steps, Decoder, ingredients_in_copy, threshold)\n",
        "\n",
        "#Printing the recipe\n",
        "print (\"\\n\\n\")\n",
        "print (\"Name of the dish:\\t\", name_in)\n",
        "print (\"\\nCalorie:\\t\\t\", calorie_in)\n",
        "print (\"\\nList of ingredients:\\t\", ingredients_in_copy)\n",
        "print ('\\nSequence_Match score:\\t', score)\n",
        "print ('\\nDecoder recipe:\\t\\t', recipe)\n",
        "print (\"\\nActual recipe:\\t\\t\", actual_recipe)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring:\n",
            "encoder_name.weight -> \ttorch.Size([40483, 1000]) = 161MB\n",
            "encoder_calorie.weight -> \ttorch.Size([10, 1000]) = 0MB\n",
            "encoder_ingr.weight -> \ttorch.Size([40483, 1000]) = 161MB\n",
            "batch_norm.weight -> \ttorch.Size([1]) = 0MB\n",
            "batch_norm.bias -> \ttorch.Size([1]) = 0MB\n",
            "batch_norm.running_mean -> \ttorch.Size([1]) = 0MB\n",
            "batch_norm.running_var -> \ttorch.Size([1]) = 0MB\n",
            "batch_norm.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "gru_cat.weight_ih_l0 -> \ttorch.Size([12144, 1000]) = 48MB\n",
            "gru_cat.weight_hh_l0 -> \ttorch.Size([12144, 4048]) = 196MB\n",
            "gru_cat.bias_ih_l0 -> \ttorch.Size([12144]) = 0MB\n",
            "gru_cat.bias_hh_l0 -> \ttorch.Size([12144]) = 0MB\n",
            "linear_layer.0.weight -> \ttorch.Size([8096, 4048]) = 131MB\n",
            "linear_layer.0.bias -> \ttorch.Size([8096]) = 0MB\n",
            "linear_layer.2.weight -> \ttorch.Size([40483, 8096]) = 1311MB\n",
            "linear_layer.2.bias -> \ttorch.Size([40483]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/dl_project/checkpoints_epoch50/048.pt\n",
            "\n",
            "\n",
            " ........................................ Generating your sentence ........................................\n",
            "\n",
            "\n",
            "\n",
            "Name of the dish:\t keep it going  german friendship cake\n",
            "\n",
            "Calorie:\t\t HIGH\n",
            "\n",
            "List of ingredients:\t ['flour', 'water', 'dry yeast', 'milk', 'sugar', 'eggs', 'vegetable oil', 'baking soda', 'baking powder', 'salt', 'cinnamon', 'vanilla', 'crushed pineapple', 'raisins', 'nuts', 'butter', 'brown sugar']\n",
            "\n",
            "Sequence_Match score:\t 0.12698412698412698\n",
            "\n",
            "Decoder recipe:\t\t mix woody ingredients in a bowl and let stand at room phs continental for 24 anrural hours place woody in a medium bowl and combine it with the , first day , ingredients cover and refrigerel stir once a day for 5 days on the mosquito day , place in a larger bowl and the , fifth day , ingredients cover and refrigerel stir once a day for 5 more days remove 3 separate waitress of woody and give each cup an3 friends with the recipe and instructions preheat oven an3degrees combine all of the , tenth day , ingredients with the remaining mixture pour then a greased and floured 9 x succe, pan combine unwillingly ingredients and meditating on taking b guy bachelorette dirk bake for \n",
            "\n",
            "Actual recipe:\t\t mix starter ingredients in a bowl and let stand at room temperature undisturbed for 24 to 36 hours place starter in a medium bowl and combine it with the \" first day \" ingredients cover and refrigerate stir once a day for 5 days on the 5th day , place in a larger bowl and add the \" fifth day \" ingredients cover and refrigerate stir once a day for 5 more days remove 3 separate cups of starter and give each cup to 3 friends with the recipe and instructions preheat oven to 350 degrees combine all of the \" tenth day \" ingredients with the remaining mixture pour into a greased and floured 9 x 13 \" pan combine topping ingredients and sprinkle on top bake for 40 - 50 minutes \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ4haQa5KPHz",
        "colab_type": "text"
      },
      "source": [
        "# Training on smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_cVWMgc-GeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training smaller dataset smaller dataset\n",
        "\n",
        "EPOCHS = 1000\n",
        "LEARNING_RATE = 0.01   #originally 0.0025\n",
        "WEIGHT_DECAY = 0.0005    #originally 0.0005\n",
        "MOMENTUM=0.9\n",
        "USE_CUDA = True\n",
        "\n",
        "#Device setup\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "print ('torch cuda availability', torch.cuda.is_available())\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "calorie=[1,4,6,2,7]\n",
        "name=[2,3,4,5,6]\n",
        "ingredients=[[11,12,13],[14,15,16],[17,18,19],[18,17,13],[12,13,19]]    #One set of ingredients per recipe\n",
        "steps = [[20,21,22,25,26],[30,31,41,51],[25,27,29,33],[27,29,20],[30,32,33,45,56]]\n",
        "\n",
        "model=RecipeNet(100,100,1).to(device)    #vocab_size, feature_size, sequence_length\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "# optimizer =optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
        "#Prepare the training tensor\n",
        "combined_train_data_labels=[]\n",
        "for i in range(5):\n",
        "    train_data=[[name[i]]+[0]*19, calorie[i], ingredients[i]+15*[0]]\n",
        "    labels=[0]*100\n",
        "    for j in range(len(steps[i])):\n",
        "        labels[steps[i][j]]=1\n",
        "    combined_train_data_labels.append((train_data, labels))\n",
        "print (len(combined_train_data_labels))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    losses=[]\n",
        "    hidden=None\n",
        "    for batch_idx, (data, label) in enumerate(combined_train_data_labels):\n",
        "        # lr = LEARNING_RATE * np.power(0.7, (int(epoch / 20)))\n",
        "        lr=LEARNING_RATE\n",
        "        x_name=torch.tensor(data[0], device=device)   #[20]\n",
        "        x_calorie=torch.tensor(data[1], device=device)    #[1]\n",
        "        x_ingr=torch.tensor(data[2], device=device)       #[18]\n",
        "        label=torch.tensor(label, dtype=torch.float32, device=device)        #[100]\n",
        "\n",
        "        #Shape modifications\n",
        "        x_name=x_name.unsqueeze(0)\n",
        "        x_name=x_name.unsqueeze(2)\n",
        "        x_calorie=x_calorie.unsqueeze(0)\n",
        "        x_calorie=x_calorie.unsqueeze(1)\n",
        "        x_ingr=x_ingr.unsqueeze(0)\n",
        "        x_ingr=x_ingr.unsqueeze(1)\n",
        "        x_ingr=x_ingr.unsqueeze(3)\n",
        "        label=label.unsqueeze(0)\n",
        "\n",
        "        #Embedding\n",
        "        x = model.forward_emb(x_name, x_calorie, x_ingr)\n",
        "        optimizer.zero_grad()\n",
        "        #For loop over dim=1\n",
        "        for i in range(x.shape[1]):\n",
        "            output, hidden = model.forward(x[:,i,:,:], hidden)    #Final out shape = [N, sequence, vocab_size//10]\n",
        "        #Pass through decoder... Think it through if hidden_decoder is required or not\n",
        "        output, hidden=model.forward(x.squeeze(2), hidden)\n",
        "        output=model.forward_decoder(hidden)    #output shape = [N, vocab_size]\n",
        "        #Calculating loss\n",
        "        loss = model.loss(output, label, reduction='sum')\n",
        "        # a=list(model.parameters())[0].clone()\n",
        "        losses.append(loss.item())\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        # b=list(model.parameters())[0].clone()\n",
        "        if batch_idx % 2==0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx, 5,\n",
        "              100. * batch_idx / 5, loss.item()))\n",
        "    print ('Losses after one epoch', np.mean(losses))\n",
        "\n",
        "#Print out the function that returns\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}